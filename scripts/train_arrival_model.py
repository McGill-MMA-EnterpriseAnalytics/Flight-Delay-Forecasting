# -*- coding: utf-8 -*-
"""Untitled39.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TF1SyZdqJiMFfx1gbL78-MihISUlMAia
"""

# train_arrival_model.py

import os
import subprocess
import argparse
import pandas as pd
import numpy as np
import joblib
import zipfile
import xgboost as xgb
import optuna
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.metrics import f1_score, make_scorer

# Function to install a package if not already installed
def install(package):
    subprocess.check_call(['pip', 'install', package])

def main(args):
    # Install required libraries
    install('kaggle')
    install('imbalanced-learn')
    install('optuna')
    install('xgboost')
    install('scikit-learn')

    from kaggle.api.kaggle_api_extended import KaggleApi

    # Authenticate Kaggle
    os.environ['KAGGLE_CONFIG_DIR'] = os.path.dirname(args.kaggle_json_path)

    api = KaggleApi()
    api.authenticate()

    # Download dataset
    print("Downloading Kaggle dataset...")
    api.dataset_download_files('patrickzel/flight-delay-and-cancellation-dataset-2019-2023', path='.', unzip=False)

    # Unzip
    print("Unzipping dataset...")
    with zipfile.ZipFile('flight-delay-and-cancellation-dataset-2019-2023.zip', 'r') as zip_ref:
        zip_ref.extractall('.')

    # Load data
    df = pd.read_csv('flights_sample_3m.csv')

    # Subset for faster processing
    df = df.sample(n=1000000, random_state=42).reset_index(drop=True)

    # Clean missing
    important_cols = ['FL_DATE', 'AIRLINE', 'ORIGIN', 'DEST', 'CRS_DEP_TIME', 'DISTANCE']
    df = df.dropna(subset=important_cols)

    # Winsorize ARR_DELAY
    lower_bound = np.percentile(df['ARR_DELAY'].dropna(), 1)
    upper_bound = np.percentile(df['ARR_DELAY'].dropna(), 99)
    df['ARR_DELAY'] = np.clip(df['ARR_DELAY'], lower_bound, upper_bound)

    # Create Target
    df['IS_ARRIVAL_DELAYED'] = (df['ARR_DELAY'] > 15).astype(int)

    # Drop leakage columns
    leakage_cols = [
        'ARR_TIME', 'ARR_DELAY', 'ELAPSED_TIME', 'AIR_TIME', 'TAXI_IN', 'WHEELS_ON',
        'CANCELLED', 'CANCELLATION_CODE', 'DIVERTED', 'DELAY_DUE_LATE_AIRCRAFT', 'AIRLINE_DOT', 'AIRLINE_CODE',
        'DELAY_DUE_CARRIER', 'DELAY_DUE_WEATHER', 'DELAY_DUE_NAS', 'DELAY_DUE_SECURITY', 'DEP_DELAY'
    ]
    df.drop(columns=[col for col in leakage_cols if col in df.columns], inplace=True)

    # Feature Engineering
    df['CRS_DEP_HOUR'] = df['CRS_DEP_TIME'] // 100
    df['CRS_DEP_MINUTE'] = df['CRS_DEP_TIME'] % 100
    df['CRS_ARR_HOUR'] = df['CRS_ARR_TIME'] // 100
    df['CRS_ARR_MINUTE'] = df['CRS_ARR_TIME'] % 100

    df['FL_DATE'] = pd.to_datetime(df['FL_DATE'])
    df['DAY_OF_WEEK'] = df['FL_DATE'].dt.dayofweek
    df['MONTH'] = df['FL_DATE'].dt.month
    df['DAY'] = df['FL_DATE'].dt.day

    df['DISTANCE_BIN'] = pd.qcut(df['DISTANCE'], q=5, labels=False)

    df.drop(['CRS_DEP_TIME', 'CRS_ARR_TIME', 'FL_DATE', 'DISTANCE'], axis=1, inplace=True)

    # Encode categoricals
    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
    for col in categorical_cols:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col].astype(str))

    X = df.drop(columns=['IS_ARRIVAL_DELAYED'])
    y = df['IS_ARRIVAL_DELAYED']

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

    # Impute missing
    imputer = SimpleImputer(strategy='mean')
    X_train = imputer.fit_transform(X_train)
    X_test = imputer.transform(X_test)

    # SMOTE
    smote = SMOTE(random_state=42)
    X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)

    # Hyperparameter tuning
    def objective(trial):
        params = {
            'max_depth': trial.suggest_int('max_depth', 4, 12),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
            'subsample': trial.suggest_float('subsample', 0.5, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
            'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.8, 2.0),
            'tree_method': 'hist',
            'device': 'cuda',
            'objective': 'binary:logistic',
            'random_state': 42,
            'eval_metric': 'logloss'
        }
        model = xgb.XGBClassifier(**params)
        score = cross_val_score(model, X_train_bal, y_train_bal, cv=3, scoring=make_scorer(f1_score)).mean()
        return score

    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=20)

    # Train best model
    best_params = study.best_params
    best_params.update({
        'tree_method': 'hist',
        'device': 'cuda',
        'objective': 'binary:logistic',
        'random_state': 42,
        'eval_metric': 'logloss'
    })
    final_model = xgb.XGBClassifier(**best_params)
    final_model.fit(X_train_bal, y_train_bal)

    # Save model
    model_filename = 'arrival_delay_xgboost_model.pkl'
    joblib.dump(final_model, model_filename)
    print(f"Model saved as {model_filename}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--kaggle_json_path', type=str, required=True)
    args = parser.parse_args()

    main(args)